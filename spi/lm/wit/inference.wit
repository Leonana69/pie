interface inference {

  use kvcache.{block-list, mask, sink};

  resource token-distribution {
    sample-p: func() -> u32;
    top-k: func(k: u32) -> list<u32>;
  }

  resource language-model {
    constructor(model-id:string);
    tokenize: func(text: string) -> list<u32>;
    detokenize: func(tokens: list<u32>) -> string;

    // for indices, we could use RLE-like encoding to compress the list.
    // dist-n is the number of distributions to return.
    predict-all: func(blocks: block-list, tokens: list<u32>, indices: list<u32>, mask:mask, sink:sink, dist-n:u32) -> list<token-distribution>;
  }
}